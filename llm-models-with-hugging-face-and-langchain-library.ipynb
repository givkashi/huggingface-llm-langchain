{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Exploring LLM Models with Hugging Face and Langchain Library : A Comprehensive Guide**","metadata":{}},{"cell_type":"markdown","source":"**> Llama, Mistral, Phi**\n\n#LLMs #HuggingFace #LangChain","metadata":{}},{"cell_type":"markdown","source":"**Step 1: Setting Up the Environment**","metadata":{}},{"cell_type":"code","source":"!pip install -q -U langchain transformers bitsandbytes accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-04T14:21:37.761657Z","iopub.execute_input":"2024-05-04T14:21:37.762388Z","iopub.status.idle":"2024-05-04T14:22:15.915757Z","shell.execute_reply.started":"2024-05-04T14:21:37.762353Z","shell.execute_reply":"2024-05-04T14:22:15.914739Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport os\nfrom langchain import PromptTemplate, HuggingFacePipeline\nfrom transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\nfrom langchain_core.prompts import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    MessagesPlaceholder,\n)\nfrom langchain_core.messages import SystemMessage","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:22:15.917596Z","iopub.execute_input":"2024-05-04T14:22:15.917914Z","iopub.status.idle":"2024-05-04T14:22:33.581187Z","shell.execute_reply.started":"2024-05-04T14:22:15.917885Z","shell.execute_reply":"2024-05-04T14:22:33.580237Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-05-04 14:22:23.320394: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-04 14:22:23.320496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-04 14:22:23.444009: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"HF_TOKEN\"]='your_huggingface_API_key'","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:22:33.582322Z","iopub.execute_input":"2024-05-04T14:22:33.582825Z","iopub.status.idle":"2024-05-04T14:22:33.587227Z","shell.execute_reply.started":"2024-05-04T14:22:33.582799Z","shell.execute_reply":"2024-05-04T14:22:33.586311Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Step 2: Initializing the Language Model**","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n# MODEL_NAME =\"mistralai/Mistral-7B-Instruct-v0.2\"\n# MODEL_NAME =\"meta-llama/Meta-Llama-3-8B\"\n# MODEL_NAME =\"microsoft/Phi-3-mini-4k-instruct\"\n# MODEL_NAME =\"microsoft/phi-1_5\"\n\n# Quantization is a technique used to reduce the memory and computation requirements\n# of deep learning models, typically by using fewer bits, 4 bits\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\n# Initialization of a tokenizer for the language model,\n# necessary to preprocess text data for input\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Initialization of the pre-trained language model\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME, torch_dtype=torch.float16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    quantization_config=quantization_config\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:22:33.589969Z","iopub.execute_input":"2024-05-04T14:22:33.590393Z","iopub.status.idle":"2024-05-04T14:24:33.086749Z","shell.execute_reply.started":"2024-05-04T14:22:33.590357Z","shell.execute_reply":"2024-05-04T14:24:33.085915Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b642096ccb274352a312ae6967a2675f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b74687fc42c4c09b51085fdff21ed60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d3f79323a6744cb910abc1c4acd121b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb5d0e9ea6b4dd3b4911a9af705eba7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a2abe66339b4c07ad5f6fbe94f42577"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84b4cc73871b45bca1ca80dec97c6cfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04635915fe2a49a7b4f00273c5a7e694"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09e673ca9b3440498389d19e0adf1a12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"189684381d4a4fee9e8ac1f5085896b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80ba41180fd45bf956836d20da69fb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99fac619b82e40b8ad57dcf491c85a41"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Step 3: Configuring Generation Settings**","metadata":{}},{"cell_type":"code","source":"# Configuration of some generation-related settings\ngeneration_config = GenerationConfig.from_pretrained(MODEL_NAME)\ngeneration_config.max_new_tokens = 1024 # maximum number of new tokens that can be generated by the model\ngeneration_config.temperature = 0.7 # randomness of the generated tex\ngeneration_config.top_p = 0 # diversity of the generated text\ngeneration_config.do_sample = True # sampling during the generation process\n# generation_config.repetition_penalty = 1.15 # the degree to which the model should avoid repeating tokens in the generated text","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:24:33.088096Z","iopub.execute_input":"2024-05-04T14:24:33.088765Z","iopub.status.idle":"2024-05-04T14:24:33.125644Z","shell.execute_reply.started":"2024-05-04T14:24:33.088722Z","shell.execute_reply":"2024-05-04T14:24:33.124782Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Step 4: Creating the Pipeline**","metadata":{}},{"cell_type":"code","source":"# A pipeline is an object that works as an API for calling the model\n# The pipeline is made of (1) the tokenizer instance, the model instance, and\n# some post-procesing settings. Here, it's configured to return full-text outputs\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=True,\n    generation_config=generation_config,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:24:33.126646Z","iopub.execute_input":"2024-05-04T14:24:33.126933Z","iopub.status.idle":"2024-05-04T14:24:33.132614Z","shell.execute_reply.started":"2024-05-04T14:24:33.126905Z","shell.execute_reply":"2024-05-04T14:24:33.131514Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# HuggingFace pipeline\nllm = HuggingFacePipeline(pipeline=pipe)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:24:33.133707Z","iopub.execute_input":"2024-05-04T14:24:33.133989Z","iopub.status.idle":"2024-05-04T14:24:33.143574Z","shell.execute_reply.started":"2024-05-04T14:24:33.133966Z","shell.execute_reply":"2024-05-04T14:24:33.142668Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Step 5: Testing the Model**","metadata":{}},{"cell_type":"code","source":"input_text = \"Write me a poem about Machine Learning.\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:24:33.144690Z","iopub.execute_input":"2024-05-04T14:24:33.145003Z","iopub.status.idle":"2024-05-04T14:24:33.156665Z","shell.execute_reply.started":"2024-05-04T14:24:33.144981Z","shell.execute_reply":"2024-05-04T14:24:33.155795Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"output = llm.invoke(input_text)\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:24:33.157901Z","iopub.execute_input":"2024-05-04T14:24:33.158296Z","iopub.status.idle":"2024-05-04T14:24:50.394242Z","shell.execute_reply.started":"2024-05-04T14:24:33.158266Z","shell.execute_reply":"2024-05-04T14:24:50.393289Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Write me a poem about Machine Learning.\n\nI'm a computer program, learning on my own\nAnalyzing data, without being shown\nA model I create, from patterns I find\nPredictions I make, with accuracy mind\n\nI learn from my mistakes, and improve each day\nThe more data I process, the better I become\nMy algorithms refine, my accuracy increases\nMy potential grows, as I learn from new insights\n\nI can recognize patterns, and make predictions with ease\nClassify data with precision, and find correlations with ease\nI can optimize processes, and improve efficiency\nMy capabilities endless, as I continue to learn with glee\n\nMachine learning, a field of endless possibilities\nA world of discovery, and new horizons\nI am a tool, to help us understand\nThe power of data, and the world around us\n\nSo let me learn, and help you in your quest\nWith my knowledge and insights, I can help you find the best\nMachine learning, a tool for the future\nA world of possibilities, and endless potential.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Step 6: Further Testing with PromptTemplate and Chain**","metadata":{}},{"cell_type":"code","source":"template = \"\"\"\n     Write me a poem about {topic}.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:24:50.395568Z","iopub.execute_input":"2024-05-04T14:24:50.396029Z","iopub.status.idle":"2024-05-04T14:24:50.400771Z","shell.execute_reply.started":"2024-05-04T14:24:50.395995Z","shell.execute_reply":"2024-05-04T14:24:50.399904Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"topic = \"Machine Learning\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:24:50.403519Z","iopub.execute_input":"2024-05-04T14:24:50.403788Z","iopub.status.idle":"2024-05-04T14:24:50.412424Z","shell.execute_reply.started":"2024-05-04T14:24:50.403765Z","shell.execute_reply":"2024-05-04T14:24:50.411654Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"prompt = PromptTemplate(input_variables=[\"topic\"], template=template)\n# Construct a Langchain Chain to connect the prompt template with the LLM\nchain = prompt | llm\noutput = chain.invoke({\"topic\": topic})\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:24:50.413511Z","iopub.execute_input":"2024-05-04T14:24:50.413842Z","iopub.status.idle":"2024-05-04T14:24:59.272738Z","shell.execute_reply.started":"2024-05-04T14:24:50.413814Z","shell.execute_reply":"2024-05-04T14:24:59.271774Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\n     Write me a poem about Machine Learning.\n\nMachine Learning\n\nA new frontier in technology\nA field that's constantly evolving\nA tool that can help us make predictions\nAnd improve our decision-making\n\nWith algorithms and models\nWe can analyze data\nAnd find patterns and insights\nThat were previously unknown\n\nFrom image recognition\nTo natural language processing\nMachine learning is changing the game\nAnd making our lives easier\n\nIt's a tool that can help us solve problems\nAnd make our world a better place\nSo let's embrace this new technology\nAnd see where it takes us.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Step 6: Further Testing with ChatPromptTemplate**","metadata":{}},{"cell_type":"code","source":"topic = \"Machine Learning\"\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=(\n                  \"\"\" Write a poem related to the input topic in one paragraph\"\"\"\n            )\n        ),\n        HumanMessagePromptTemplate.from_template(\"```{topic}```\"),\n    ]\n)\n\nchain = prompt | llm\noutput = chain.invoke({\"topic\": topic})\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T14:24:59.274091Z","iopub.execute_input":"2024-05-04T14:24:59.274367Z","iopub.status.idle":"2024-05-04T14:25:11.467040Z","shell.execute_reply.started":"2024-05-04T14:24:59.274344Z","shell.execute_reply":"2024-05-04T14:25:11.466079Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"System:  Write a poem related to the input topic in one paragraph\nHuman: ```Machine Learning```\n\nA human mind is like a machine,\nLearning and growing with each experience.\nWith every new input, the mind processes and stores,\nImproving with each passing hour.\n\nLike a machine, the human brain can learn,\nFrom past experiences and future concerns.\nIt can adapt and change with each new task,\nAnd become more efficient with each new ask.\n\nMachine learning is a process of human learning,\nA way to teach machines to learn and yearn.\nWith each new input, the machine improves,\nAnd becomes more efficient with each new move.\n\nHuman and machine, learning together,\nA partnership that will forever endure.\nWith each new input, the mind and machine,\nWill learn and grow, in perfect harmony.\n","output_type":"stream"}]}]}